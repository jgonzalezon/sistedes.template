% Este archivo es ejemplo-articulo-regular-es.tex, un capítulo
% de ejemplo (en español) del paquete 'sistedes' para la
% Biblioteca Digital de Sistedes; Version 1.2 of 2023/11/09
%
% Este paquete extiende el paquete LLNCS de Springer Computer
% Science proceedings.
\documentclass{sistedes}

\usepackage[spanish,es-tabla]{babel}

\usepackage{graphicx}

% Usado para mostrar la figura de ejemplo. 
%
\begin{document}
%
\title{Viabilidad de Large Language Models ejecutados en dispositivos on-edge para asistencia administrativa en el sector agroalimentario}

%
%\titlerunning{Título abreviado}
% Si el título del artículo es demasiado largo para el encabezado,
% puede establecer un título de artículo abreviado aquí
%
\author{Francisco Javier González Ontañón\inst{1}\orcidID{0000-1111-2222-3333} \and
Segundo Autor\inst{2,3}\orcidID{1111-2222-3333-4444} \and
Tercer Autor\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Autor et al.}
% Los nombres se abrevian en el encabezado.
% Si hay más de dos autores, se usa 'et al.'
%
\institute{
Unizar, Zaragoza, Spain\\
\email{primer@example.com}\\
}

%
\maketitle              % Componer el encabezado de la contribución
%
\begin{abstract}
Los Large Language Models (LLMs) han demostrado un rendimiento sobresaliente en tareas de procesamiento del lenguaje natural cuando se ejecutan en infraestructuras cloud. Sin embargo, su dependencia de conectividad permanente, los requisitos de latencia y las preocupaciones relacionadas con la privacidad limitan su adopción en contextos profesionales que operan en entornos con recursos restringidos. En este trabajo se presenta una investigación en curso que analiza la viabilidad de ejecutar LLMs directamente en dispositivos on-edge, como teléfonos móviles, para asistir en tareas administrativas del sector agroalimentario.

El artículo se centra en un caso de uso real: el apoyo al rellenado de documentación agrícola oficial en entornos rurales con conectividad limitada. Se discuten las restricciones impuestas por el hardware disponible, así como las limitaciones actuales de las herramientas de ingeniería del software para desplegar y evaluar este tipo de soluciones. Más que proponer una solución cerrada, el trabajo identifica problemas abiertos relacionados con el diseño, la evaluación y la integración de LLMs on-edge en aplicaciones reales, con el objetivo de fomentar la discusión y servir como punto de partida para futuras investigaciones y colaboraciones.

\keywords{LLMs on-edge \and ingeniería del software \and dispositivos móviles \and sector agroalimentario}

\end{abstract}
%
%
%
\section{Introducción}

Los Large Language Models (LLMs) están impulsando una nueva generación de sistemas capaces de comprender y generar lenguaje natural, pero su despliegue práctico sigue estando condicionado por requisitos computacionales y de memoria elevados. En particular, la ejecución en dispositivos móviles plantea un conjunto de restricciones específicas (memoria disponible, latencia, longitud de entrada y motor de inferencia), cuya caracterización empírica en escenarios reales se ha abordado mediante estudios de medida y análisis comparativo \cite{li_large_2024,yan_are_2025}. Estos trabajos muestran que, aunque la ejecución local es posible en determinadas condiciones, el rendimiento y la calidad dependen fuertemente del tamaño del modelo y de las optimizaciones aplicadas.

En paralelo, la comunidad está convergiendo hacia el paradigma \emph{on-edge}/\emph{on-device} como alternativa (o complemento) a arquitecturas basadas en la nube, motivada por la reducción de latencia, la resiliencia ante conectividad intermitente y la necesidad de mantener los datos cerca de su origen. Diversas revisiones recientes sintetizan técnicas y desafíos para habilitar inferencia eficiente en dispositivos con recursos limitados, destacando estrategias basadas en modelos más pequeños, compresión y optimización de inferencia \cite{xu_-device_2024,wang_empowering_2025,friha_llm-based_2024}. No obstante, estas mismas revisiones subrayan problemas abiertos de ingeniería relacionados con evaluación reproducible, integración en aplicaciones reales y consideraciones de seguridad y confiabilidad.

Para aproximar los LLMs a escenarios restringidos se han propuesto familias de técnicas de compresión y ajuste eficiente, incluyendo cuantización, poda, distilación y enfoques de \emph{parameter-efficient fine-tuning} (PEFT), junto con taxonomías sistemáticas que evidencian la falta de marcos unificados que integren estas decisiones de diseño en pipelines de despliegue completos \cite{kim_efficient_2025}. En el contexto móvil, se exploran tanto arquitecturas sub-billion optimizadas para uso local \cite{liu_mobilellm_2024} como técnicas de cuantización específicamente orientadas a hardware móvil \cite{tan_mobilequant_2024}. Además, existen implementaciones tempranas que materializan estos enfoques en prototipos de aplicaciones Android on-device, aportando evidencias prácticas sobre ventajas y limitaciones del despliegue local \cite{bagawan_develop_2024}.

En este contexto, el presente artículo en curso investiga la viabilidad de aplicar LLMs ejecutados completamente en el dispositivo a un escenario real del sector agroalimentario, centrado en asistencia para tareas administrativas y documentación oficial en entornos con conectividad limitada. El objetivo es identificar retos de ingeniería del software (diseño, integración, evaluación y mantenimiento) y delimitar problemas abiertos que orienten futuras colaboraciones y líneas de investigación.





\section{Caso de uso y motivación}

El sector agroalimentario requiere el cumplimiento de una amplia variedad de obligaciones administrativas, como el registro de labores agrícolas, tratamientos fitosanitarios o inspecciones técnicas. Estas tareas se realizan habitualmente en el propio campo, utilizando dispositivos móviles y, en muchos casos, sin acceso fiable a Internet.

La posibilidad de contar con un asistente basado en lenguaje natural que funcione completamente en el dispositivo podría reducir errores, agilizar la introducción de datos y mejorar el cumplimiento normativo. Sin embargo, las soluciones actuales suelen depender de servicios cloud, lo que limita su aplicabilidad en este contexto y plantea problemas adicionales de privacidad y soberanía del dato.

Este escenario permite identificar de forma clara las tensiones entre las capacidades de los LLMs y las restricciones reales del entorno de ejecución.

\section{Problemas abiertos y retos de ingeniería}

La ejecución de LLMs en dispositivos on-edge introduce una serie de problemas abiertos que aún no están adecuadamente resueltos por las herramientas existentes. Entre ellos destacan la selección y adaptación de modelos de tamaño reducido, la gestión eficiente de memoria y energía, y la evaluación de la calidad de las respuestas en condiciones de recursos limitados.

Desde el punto de vista de la ingeniería del software, se observa una falta de metodologías estandarizadas para diseñar, desplegar y evaluar aplicaciones basadas en LLMs on-edge. Los benchmarks disponibles suelen centrarse en métricas aisladas y no reflejan adecuadamente escenarios de uso reales. Asimismo, la integración de estos modelos en aplicaciones móviles plantea desafíos adicionales relacionados con mantenimiento, actualización y depuración.

Estos problemas sugieren la necesidad de enfoques específicos que tengan en cuenta tanto las restricciones técnicas como los requisitos del dominio.

\section{Metodología y estado actual del trabajo}

El trabajo se encuentra en una fase inicial de desarrollo. Actualmente se está llevando a cabo una revisión del estado del arte y una captura de requisitos mediante cuestionarios dirigidos a profesionales del sector agroalimentario. A partir de estos requisitos se prevé el desarrollo de un prototipo experimental que permita evaluar distintas configuraciones de modelos y técnicas de optimización.

La evaluación se realizará mediante benchmarks específicos del dominio, considerando métricas como latencia, uso de memoria y adecuación funcional. El objetivo no es ofrecer una solución definitiva, sino identificar patrones, limitaciones y oportunidades de mejora que puedan orientar futuras investigaciones.

\section{Conclusiones preliminares y líneas futuras}

Este trabajo pone de manifiesto que la adopción de LLMs on-edge en aplicaciones reales plantea desafíos que van más allá del rendimiento del modelo, afectando directamente a decisiones de diseño y a la disponibilidad de herramientas de ingeniería adecuadas. El caso de uso presentado evidencia la necesidad de enfoques específicos para dominios con restricciones severas de recursos y alta sensibilidad de los datos.

Como trabajo en curso, este artículo pretende fomentar la discusión en la comunidad de Ingeniería del Software y Bases de Datos, y servir como punto de partida para futuras colaboraciones orientadas a cerrar la brecha entre los avances en modelos de lenguaje y su aplicación práctica en entornos on-edge.



\begin{credits}
\subsubsection{\ackname} 
Agradecimientos
\subsubsection{\discintname}
El autor declara que no tiene ningún interés financiero ni relación personal que pudiera influir en el trabajo descrito en este artículo.
\end{credits}
%
% ---- Bibliografía ----
%
% Los usuarios de BibTeX deben especificar el estilo de bibliografía 'splncs04'.
% Las referencias se ordenarán y formatearán con el estilo correcto.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\bibliographystyle{splncs04}
\bibliography{referencias}

\end{document}
